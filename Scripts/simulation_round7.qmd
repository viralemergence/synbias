---
title: "synbias simulations: Round 7"
author: "July Pilowsky"
format: html
---

# Goal

To run simulations of coinfection in a host population and determine how interactions between coinfecting strains affect detectability of these strains through imperfect sampling. This round of simulations will include full stratified sampling of all epidemiological parameters, false negative and false positive rates, as well as the balanced sampling of possible species interactions found in previous rounds. This round will be run in Julia using my package `CoinfectionSimulator.jl`.

```{julia setup}
using CoinfectionSimulator
using LatinHypercubeSampling
using DataFrames
using Random
using Distributions
using CSV
using Tables
using Tidier
using MLJ
using TidierPlots
using BetaML
using Shapley
using CategoricalArrays
using Arrow
Random.seed!(18)
n_sims = 10000
```

# Simulation setup

## Parameters

The following parameters will change across simulations and will be sampled in a stratified manner:

1.  Interaction strength (between strains), 0 - 0.5
2.  Competition:facilitation ratio, 0.2 - 0.8
3.  Number of strains, 5 - 50
4.  Proportion of the population sampled, 0 - 1
5.  False negative rate, 0 - 0.5
6.  Disease-induced mortality, 0 - 0.5
7.  Transmission, 0 - 0.1
8.  Latency, 1 - 20
9.  Recovery, 0 - 0.1

```{julia variables}
plan, _ = LHCoptim(n_sims, 9, 10)
scaled_plan = scaleLHC(plan, [(0, 0.5), (0.2, 0.8), (5, 50), (0, 1),
    (0, 0.5), (0, 0.5), (0, 0.1), (1, 20), (0, 0.1)])
```

The following parameters will remain constant across all simulations:

1.  Priority effects (all simulations will have priority effects)
2.  False positive rate (0 across all simulations)
3.  Initial population size (1000 individuals)
4.  Baseline host mortality (always 0)
5.  Host fecundity (always 0)
6.  Number of time steps (always 100)
7.  Introduction of strains (always simultaneous at the first timestep)

## Prep interaction matrices

I can create interaction matrices for all the simulations using the `prep_interaction_matrix` function from `CoinfectionSimulator.jl`.

```{julia prep interaction matrices}
input_df = DataFrame(
    interaction_strength=scaled_plan[:, 1],
    cf_ratio=scaled_plan[:, 2],
    priority_effects=trues(n_sims),
    strains=round.(Int, scaled_plan[:, 3]),
    proportion_sampled=scaled_plan[:, 4],
    false_negative=scaled_plan[:, 5],
    disease_mortality=scaled_plan[:, 6],
    transmission=scaled_plan[:, 7],
    latency=scaled_plan[:, 8],
    recovery=scaled_plan[:, 9],
)
CSV.write("Data/simulation_round7_input.csv", input_df)
matrices = create_interaction_matrix(input_df)
```

# Simulation

## SEIR simulations

```{julia simulate_seir}
input_df = CSV.read("Data/simulation_round7_input.csv", DataFrame)
matrices = create_interaction_matrix(input_df)
n_individuals = 1000

# Collect detection vectors
results = zeros(n_sims, 100)

# Simulate
for i in 1:n_sims
    # Create SEIR strains
    n_strains = Int(input_df.strains[i])
    strains = Vector{SEIRModel}(undef, n_strains)
    for i in 1:n_strains
        strains[i] = SEIRModel(
            rand(Truncated(Normal(input_df.transmission[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.disease_mortality[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.recovery[i], 1), 0, 1)),
            round(Int, rand(Truncated(Normal(input_df.latency[i], 1), 1, 10)))
        )
    end
    # Initialize population - everyone starts susceptible
    initial_pop = Population(Individual[])
    for i in 1:n_individuals
        individual = Individual(n_strains, 1)  # age 1
        push!(initial_pop.individuals, individual)
    end
    # Set simulation parameters
    params = SimulationParameters(
        strains,
        matrices[i],
        0.0, # base mortality
        0.0, # fecundity
        1, # Age of maturity
        :simultaneous, # introduction of strains
        100, # time steps
        :density # transmission type
    )

    true_pop = simulate(initial_pop, params)

    sampling_params = SamplingParameters(
        input_df.proportion_sampled[i],
        0.0, # false positive
        input_df.false_negative[i],
    )

    detections = sample_populations(true_pop, sampling_params)

    results[i, :] = sum(detections, dims=2)
end

CSV.write("Data/simulation_round7_output.csv",
    Tables.table(results),
    writeheader=false)
```

## SI simulations

```{julia simulate_si}
input_df = CSV.read("Data/simulation_round7_input.csv", DataFrame)
matrices = create_interaction_matrix(input_df)
n_individuals = 1000

# Collect detection vectors
si_results = zeros(n_sims, 100)

# Simulate
for i in 1:n_sims
    # Create SI strains
    n_strains = Int(input_df.strains[i])
    strains = Vector{SIModel}(undef, n_strains)
    for i in 1:n_strains
        strains[i] = SIModel(
            rand(Truncated(Normal(input_df.transmission[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.disease_mortality[i], 1), 0, 1))
        )
    end
    # Initialize population - everyone starts susceptible
    initial_pop = Population(Individual[])
    for i in 1:n_individuals
        individual = Individual(n_strains, 1)  # age 1
        push!(initial_pop.individuals, individual)
    end
    # Set simulation parameters
    params = SimulationParameters(
        strains,
        matrices[i],
        0.0, # base mortality
        0.0, # fecundity
        1, # Age of maturity
        :simultaneous, # introduction of strains
        100, # time steps
        :frequency # transmission type
    )

    true_pop = simulate(initial_pop, params)

    sampling_params = SamplingParameters(
        input_df.proportion_sampled[i],
        0.0, # false positive
        input_df.false_negative[i],
    )

    detections = sample_populations(true_pop, sampling_params)

    si_results[i, :] = sum(detections, dims=2)
end

CSV.write("Data/round7_si_output.csv",
    Tables.table(si_results),
    writeheader=false)
```

# Analysis

## Distribution of Error rates

```{julia analysis dataset}
# Read the data back in
input_parameters = @chain CSV.read("Data/simulation_round7_input.csv", DataFrame) begin
    @mutate(sim = 1:10000) # simulation IDs
end
simulation_output_seir = @chain CSV.read("Data/simulation_round7_output.csv", DataFrame, header=false) begin
    @mutate(sim = 1:10000) # simulation IDs
    @pivot_longer(cols = 1:100,
        names_to = "timestep",
        values_to = "Detections")
    transform!(:timestep => ByRow(x -> parse(Int, replace(x, "Column" => ""))) => :timestep)
    @left_join(input_parameters, sim) # add input parameters
    @mutate(error_rate = abs(strains - Detections) / strains) # calculate error rate
end

for col in names(simulation_output_seir)
    if eltype(simulation_output_seir[!, col]) >: Missing
        # Remove Missing from the column type
        simulation_output_seir[!, col] = identity.(simulation_output_seir[!, col])
    end
end

ggplot(simulation_output_seir) +
    geom_histogram(aes(x=:error_rate), bins=50) +
    scale_y_log10()
```

This is like a negative exponential distribution of error rates for the density-dependent SEIR simulations.

Let's see how this looks for the SI simulation results:

```{julia histogram_si}
input_parameters = @chain CSV.read("Data/simulation_round7_input.csv", DataFrame) begin
    @mutate(sim = 1:10000) # simulation IDs
end
simulation_output_si = @chain CSV.read("Data/round7_si_output.csv", DataFrame, header=false) begin
    @mutate(sim = 1:10000) # simulation IDs
    @pivot_longer(cols = 1:100,
        names_to = "timestep",
        values_to = "Detections")
    transform!(:timestep => ByRow(x -> parse(Int, replace(x, "Column" => ""))) => :timestep)
    @left_join(input_parameters, sim) # add input parameters
    @mutate(error_rate = abs(strains - Detections) / strains) # calculate error rate
end

for col in names(simulation_output_si)
    if eltype(simulation_output_si[!, col]) >: Missing
        # Remove Missing from the column type
        simulation_output_si[!, col] = identity.(simulation_output_si[!, col])
    end
end

ggplot(simulation_output_si) +
    geom_histogram(aes(x=:error_rate), bins=50) +
    scale_y_log10()
```

It seems that most of the samples result in no detections, which is to say an error rate of 1. I believe that this is due to the frequency-dependent nature of this simulation, based on my experimentation with different simulation settings.

## Sensitivity Analysis

Now I will use a machine learning algorithm to find which input parameters affect the error rate the most. First I will do it for the SEIR simulations:

```{r sensitivity_analysis_seir}
library(data.table)
library(here)
library(tidymodels)
library(stringr)
library(vip)
library(future)
plan(multicore)

input_parameters <- fread(here("Data/simulation_round7_input.csv"))[, -c("priority_effects")][, sim := 1:10000]
simulation_output_seir <- fread(here("Data/simulation_round7_output.csv"))[, sim := 1:10000] |>
    melt(data = _, id.vars = "sim", measure_vars = 1:100, variable.name = "timestep", value.name = "Detections") |>
    _[, timestep := as.numeric(str_sub(timestep, 2))] |>
    merge(x = _, y = input_parameters, by = "sim") |>
    _[, error_rate := abs(strains - Detections) / strains]
synbias_split <- initial_split(simulation_output_seir[, -c("sim", "Detections")], strata = error_rate)
synbias_train <- training(synbias_split)
synbias_test <- testing(synbias_split)
tune_spec_seir <- boost_tree(
    # trees = tune(),
    # min_n = tune(),
    # learn_rate = tune(),
    # tree_depth = tune(),
    # loss_reduction = tune(),
    # sample_size = tune(),
    # mtry = tune(),
) |>
    set_engine("xgboost") |>
    set_mode("regression")
# synbias_folds <- vfold_cv(synbias_train, 4)
# synbias_res <- workflow() |>
#     add_model(tune_spec) |>
#     add_formula(error_rate ~ cf_ratio * interaction_strength + timestep + strains + proportion_sampled + false_negative + disease_mortality + transmission + latency + recovery) |>
#     tune_grid(
#         resamples = synbias_folds
#     )
# best_tree <- synbias_res |> select_best(metric = "rmse") # trees = 1555, min_n = 23, learn_rate = 0.316, tree_depth = 10, loss_reduction = 1.90e-9, sample_size = 1, mtry = 8
best_tree_seir <- list(
    trees = 1555,
    min_n = 23,
    learn_rate = 0.316,
    tree_depth = 10,
    loss_reduction = 1.90e-9,
    sample_size = 1,
    mtry = 8,
    stop_iter = 1000
)
final_wf_seir <- workflow() |>
    add_model(tune_spec_seir) |>
    add_formula(error_rate ~ cf_ratio * interaction_strength + timestep + strains + proportion_sampled + false_negative + disease_mortality + transmission + latency + recovery) |>
    finalize_workflow(best_tree_seir)
final_fit_seir <- final_wf_seir %>%
    last_fit(synbias_split)
final_fit_seir %>%
    collect_metrics()
final_fit_seir |>
    extract_workflow() |>
    extract_fit_parsnip() %>%
    vip()
```

In the end, a very interesting result. The strong effects of timestep, number of strains, false negative rate, and proportion sampled were expected. It's interesting that C:F ratio is more important to the error rate than transmission or recovery.

```{r sensitivity_analysis_si}
library(data.table)
library(here)
library(tidymodels)
library(stringr)
library(vip)
library(future)
plan(multicore)

input_parameters <- fread(here("Data/simulation_round7_input.csv"))[, -c("priority_effects", "recovery", "latency")][, sim := 1:10000]
simulation_output_si <- fread(here("Data/round7_si_output.csv"))[, sim := 1:10000] |>
    melt(data = _, id.vars = "sim", measure_vars = 1:100, variable.name = "timestep", value.name = "Detections") |>
    _[, timestep := as.numeric(str_sub(timestep, 2))] |>
    merge(x = _, y = input_parameters, by = "sim") |>
    _[, error_rate := abs(strains - Detections) / strains]
si_split <- initial_split(simulation_output_si[, -c("sim", "Detections")], strata = error_rate)
si_train <- training(si_split)
si_test <- testing(si_split)
tune_spec_si <- boost_tree(
    trees = tune(),
    min_n = tune(),
    learn_rate = tune(),
    tree_depth = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry = tune()
) |>
    set_engine("xgboost") |>
    set_mode("regression")
si_folds <- vfold_cv(si_train, 4)
si_res <- workflow() |>
    add_model(tune_spec_si) |>
    add_formula(error_rate ~ cf_ratio * interaction_strength + timestep + strains + proportion_sampled + false_negative + disease_mortality + transmission) |>
    tune_grid(
        resamples = si_folds,
        control = control_grid(verbose = T)
    )
best_tree_si <- si_res |> select_best(metric = "rmse")
```

Let's examine these results more closely using plots of Shapley values.

```{r shapley_plots}
# Create the model matrix using the same formula as workflow
formula_seir <- error_rate ~ cf_ratio * interaction_strength + timestep + strains + proportion_sampled + false_negative + disease_mortality + transmission + latency + recovery
train_matrix_seir <- si_test |>
    slice_sample(prop = 0.1) |>
    model.matrix(object = formula, data = _) |>
    _[, -1] # remove intercept
var_names <- c("C:F Ratio", "Interaction Strength", "Timestep", "Strains", "Prop. Sampled", "False Negative", "Disease Mortality", "Transmission", "Latency", "Recovery", "CF Ratio x Strength")

# Extract the xgboost model
xgb_model_seir <- final_fit_seir %>%
    extract_workflow() %>%
    extract_fit_parsnip() %>%
    .$fit

# Compute SHAP values
library(SHAPforxgboost)
shapley_data_seir <- shap.values(xgb_model_seir, as.matrix(train_matrix_seir))
shap_long_seir <- shap.prep(shap_contrib = shapley_data_seir$shap_score, X_train = train_matrix_seir)
shap.plot.summary(shap_long_seir)

source(here("Scripts/shapley_plots.R"))
library(patchwork)
partial.dependence.plot(colnames(train_matrix_seir), shap_long_seir, as.data.frame(train_matrix_seir), c("#4F6980FF", "#849DB1FF", "#A2CEAAFF", "#638B66FF", "#BFBB60FF", "#F47942FF", "#FBB04EFF", "#B66353FF", "#D7CE9FFF", "#B9AA97FF", "#7E756DFF"), var_names)
```

# Accumulation Curves

Now I will make figures of accumulation curves for each set of simulations: a species accumulation curve and an interaction accumulation curve.

## Species Accumulation Curves

```{julia species_accumulation_curves}
input_df = CSV.read("Data/simulation_round7_input.csv", DataFrame)
matrices = create_interaction_matrix(input_df)
n_individuals = 1000

# Collect detection vectors
cumulative_detections = zeros(n_sims)

# Simulate
for i in 1:n_sims
    # Create SEIR strains
    n_strains = Int(input_df.strains[i])
    strains = Vector{SEIRModel}(undef, n_strains)
    for i in 1:n_strains
        strains[i] = SEIRModel(
            rand(Truncated(Normal(input_df.transmission[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.disease_mortality[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.recovery[i], 1), 0, 1)),
            round(Int, rand(Truncated(Normal(input_df.latency[i], 1), 1, 10)))
        )
    end
    # Initialize population - everyone starts susceptible
    initial_pop = Population(Individual[])
    for i in 1:n_individuals
        individual = Individual(n_strains, 1)  # age 1
        push!(initial_pop.individuals, individual)
    end
    # Set simulation parameters
    params = SimulationParameters(
        strains,
        matrices[i],
        0.0, # base mortality
        0.0, # fecundity
        1, # Age of maturity
        :simultaneous, # introduction of strains
        100, # time steps
        :density # transmission type
    )

    true_pop = simulate(initial_pop, params)

    sampling_params = SamplingParameters(
        input_df.proportion_sampled[i],
        0.0, # false positive
        input_df.false_negative[i],
    )

    detections = sample_populations(true_pop, sampling_params)

    detection_by_strain = sum(detections, dims=1) .> 1

    cumulative_detections[i] = sum(detection_by_strain)/n_strains
end

cumul_df = DataFrame(sim = 1:10000, cumulative_detections = cumulative_detections)
interaction_scores = CSV.read("Data/interaction_scores.csv", DataFrame)

cumulative_detections_seir = @chain CSV.read("Data/simulation_round7_input.csv", DataFrame) begin
    @mutate(sim = 1:10000) # simulation IDs
    @left_join(cumul_df, sim)
    @left_join(interaction_scores, sim)
    @mutate(
        competition_score = total_competition/strains,
        facilitation_score = total_facilitation/strains
    )
    @mutate(
        cf_cat = case_when(
            facilitation_score - competition_score <= -1 => "high competition",
            facilitation_score - competition_score <= 1 => "neutral", 
            facilitation_score - competition_score > 1 => "high facilitation"
        )
    )
end

cumulative_detections_seir.cf_cat = CategoricalArray(cumulative_detections_seir.cf_cat, 
                                   levels = ["high competition", "neutral", "high facilitation"],
                                   ordered = true)

p1 = ggplot(cumulative_detections_seir, @aes(x = proportion_sampled, y = cumulative_detections, color = cf_cat)) +
    geom_point(alpha = 0.1) +
    geom_smooth(linewidth = 20) +
    scale_y_continuous(name = "Proportion of Parasite Species Detected") +
    scale_x_continuous(name = "Proportion of Host Population Sampled") +
    labs(color = "Interaction Type", title = "Species Accumulation Curve: SEIR model") +
    scale_color_manual(values = ["#BB5566FF", "#DDAA33FF", "#004488FF"])

CSV.write("Data/seir_cumulative_detection.csv", cumulative_detections_seir)

cumulative_detections_si = zeros(n_sims)

for i in 1:n_sims
    # Create SI strains
    n_strains = Int(input_df.strains[i])
    strains = Vector{SIModel}(undef, n_strains)
    for i in 1:n_strains
        strains[i] = SIModel(
            rand(Truncated(Normal(input_df.transmission[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.disease_mortality[i], 1), 0, 1))
        )
    end
    # Initialize population - everyone starts susceptible
    initial_pop = Population(Individual[])
    for i in 1:n_individuals
        individual = Individual(n_strains, 1)  # age 1
        push!(initial_pop.individuals, individual)
    end
    # Set simulation parameters
    params = SimulationParameters(
        strains,
        matrices[i],
        0.0, # base mortality
        0.0, # fecundity
        1, # Age of maturity
        :simultaneous, # introduction of strains
        100, # time steps
        :frequency # transmission type
    )

    true_pop = simulate(initial_pop, params)

    sampling_params = SamplingParameters(
        input_df.proportion_sampled[i],
        0.0, # false positive
        input_df.false_negative[i],
    )

    detections = sample_populations(true_pop, sampling_params)

    detection_by_strain = sum(detections, dims=1) .> 1

    cumulative_detections_si[i] = sum(detection_by_strain)/n_strains
end

cumul_df = DataFrame(sim = 1:10000, cumulative_detections = cumulative_detections_si)

cumulative_detections_si = @chain CSV.read("Data/simulation_round7_input.csv", DataFrame) begin
    @mutate(sim = 1:10000) # simulation IDs
    @left_join(cumul_df, sim)
    @left_join(interaction_scores, sim)
    @mutate(
        competition_score = total_competition/strains,
        facilitation_score = total_facilitation/strains
    )
    @mutate(
        cf_cat = case_when(
            facilitation_score - competition_score <= -1 => "high competition",
            facilitation_score - competition_score <= 1 => "neutral", 
            facilitation_score - competition_score > 1 => "high facilitation"
        )
    )
end

cumulative_detections_si.cf_cat = CategoricalArray(cumulative_detections_si.cf_cat, 
                                   levels = ["high competition", "neutral", "high facilitation"],
                                   ordered = true)

p2 = ggplot(cumulative_detections_si, @aes(x = proportion_sampled, y = cumulative_detections, color = cf_cat)) +
    geom_point(alpha = 0.1) +
    geom_smooth(linewidth = 3) +
    scale_y_continuous(name = "Proportion of Parasite Species Detected") +
    scale_x_continuous(name = "Proportion of Host Population Sampled") +
    labs(color = "Interaction Type", title = "Species Accumulation Curve: SI model") +
    scale_color_manual(values = ["#BB5566FF", "#DDAA33FF", "#004488FF"])

CSV.write("Data/si_cumulative_detection.csv", cumulative_detections_si)

ggsave(p2, "Figures/Fig2b_si_species.png")
ggsave(p1, "Figures/Fig2a_seir_species.png")
```

The results don't amount to much, mainly because cutting up the C:F ratio into gross categories like this doesn't really capture the non-linear relationship between C:F ratio, interaction strength, and error rate that you can observe in the Shapley plots above.

## Interaction Accumulation Curves

This will be a little trickier to implement because the software library for estimating species interactions doesn't exist in Julia as far as I know, so I'm going to have to pass the results from the simulations in Julia in an appropriate format to R, where I can do the cooccurrence analysis.

```{julia simulate_communities}
input_df = CSV.read("Data/simulation_round7_input.csv", DataFrame)
matrices = create_interaction_matrix(input_df)
n_individuals = 1000
n_timesteps = 100

results = Vector{Vector{BitMatrix}}(undef, n_sims)

# Simulate
for i in 1:n_sims
    # Create SEIR strains
    n_strains = Int(input_df.strains[i])
    strains = Vector{SEIRModel}(undef, n_strains)
    for i in 1:n_strains
        strains[i] = SEIRModel(
            rand(Truncated(Normal(input_df.transmission[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.disease_mortality[i], 1), 0, 1)),
            rand(Truncated(Normal(input_df.recovery[i], 1), 0, 1)),
            round(Int, rand(Truncated(Normal(input_df.latency[i], 1), 1, 10)))
        )
    end
    # Initialize population - everyone starts susceptible
    initial_pop = Population(Individual[])
    for i in 1:n_individuals
        individual = Individual(n_strains, 1)  # age 1
        push!(initial_pop.individuals, individual)
    end
    # Set simulation parameters
    params = SimulationParameters(
        strains,
        matrices[i],
        0.0, # base mortality
        0.0, # fecundity
        1, # Age of maturity
        :simultaneous, # introduction of strains
        n_timesteps,
        :density # transmission type
    )

    true_pop = simulate(initial_pop, params)

    timestep_matrices = Vector{BitMatrix}()
    for pop in true_pop
        if !isempty(pop.individuals)
            timestep = hcat([ind.state[:, 3] for ind in pop.individuals]...)
            push!(timestep_matrices, timestep)
        end
    end

    results[i] = timestep_matrices
end

for (sim_idx, sim_data) in enumerate(results)
    for (timestep_idx, matrix) in enumerate(sim_data)
        int_matrix = Int.(Array(matrix))
        df = DataFrame(int_matrix, :auto)
        Arrow.write("Data/seir_site_species_matrices/sim_$(sim_idx)_timestep_$(timestep_idx).arrow",
            df)
    end
end
```

Now that I have the site/species matrices needed for cooccurrence analysis, I'll open them in R and do just that.

```{r cooccurrence_analysis}
library(tidyverse)
library(arrow)
library(cooccur)
quiet <- function(x) {
  sink(tempfile())
  on.exit(sink())
  invisible(force(x))
}

handle_site_species_matrix <- function(file) {
  m <- read_feather(file) |> as.matrix()
  tryCatch(
    {
      cooccurrence_analysis <- quiet(cooccur(
        test_matrix,
        thresh = T,
        spp_names = FALSE,
        prob = "comb"
      ))
    },
    error = function(e) {
      return(NA)
    }
  )
  competitive_pairs <- cooccurrence_analysis$results |>
    filter(exp_cooccur >= 1, p_lt < 0.05) |>
    nrow()
  facilitative_pairs <- cooccurrence_analysis$results |>
    filter(exp_cooccur >= 1, p_gt < 0.05) |>
    nrow()
  return(tibble(
    file = file,
    competitive_pairs = competitive_pairs,
    facilitative_pairs = facilitative_pairs
  ))
}
test_matrix <- handle_site_species_matrix(
  "Data/seir_site_species_matrices/sim_1_timestep_1.arrow"
)
```